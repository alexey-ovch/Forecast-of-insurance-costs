# Прогноз расходов на страхование

Проект посвящен разработке модели машинного обучения для предсказания величины расходов на медицинское страхование по характеристикам клиентов. Задача является задачей регрессии с метрикой **RMSLE** (Root Mean Squared Logarithmic Error).

## Датасет
Данные представлены в виде CSV-файлов:
- **train.csv** — обучающая выборка с таргет-переменной
- **test.csv** — тестовая выборка для генерации предсказаний

### Признаки
Модель использует следующие признаки:

**Числовые признаки:**
- `age` — возраст клиента
- `bmi` — индекс массы тела

**Категориальные признаки:**
- `sex` — пол (male/female)
- `smoker` — курящий/некурящий (yes/no)
- `region` — регион проживания (northeast/northwest/southeast/southwest)
- `children` — количество детей

**Целевая переменная:**
- `charges` — сумма страховых взносов (таргет)

## Предобработка данных

### 1. Кодирование категориальных переменных

Применяется **one-hot encoding** для категориальных переменных `sex`, `smoker`, `region`: pd.get_dummies(train[["sex", "smoker", "region"]])

Результат: создание бинарных признаков:
- `sex_female`, `sex_male`
- `smoker_no`, `smoker_yes`
- `region_northeast`, `region_northwest`, `region_southeast`, `region_southwest`

### 2. Нормализация числовых признаков

Числовые признаки `age` и `bmi` стандартизируются с помощью **z-score**: train[numerical_columns].apply(sp.stats.zscore)


Это приводит признаки к стандартному нормальному распределению (среднее = 0, стандартное отклонение = 1).

### 3. Преобразование таргета

Таргет-переменная `charges` логарифмируется для уменьшения влияния выбросов и лучшей работы модели: train_y = np.log1p(train["charges"])


Используется `log1p` (ln(1+x)) для корректной обработки нулевых значений.

### 4. Удаление ненужных столбцов

Удаляются исходные категориальные столбцы и `id` после кодирования.

## Модель машинного обучения

### Алгоритм

**GradientBoostingRegressor** — градиентный бустинг над деревьями решений. Этот алгоритм последовательно строит деревья, каждое из которых исправляет ошибки предыдущих.

### Гиперпараметры

params = {
'n_estimators': 120,
'learning_rate': 0.04
}

- `n_estimators = 120` — количество деревьев в ансамбле
- `learning_rate = 0.04` — темп обучения (скорость сходимости, меньшее значение = более осторожное обучение)

### Кросс-валидация

Используется **K-Fold Cross-Validation** с 11 фолдами: kf = KFold(n_splits=11, shuffle=True, random_state=422)

**Процесс кросс-валидации:**
- Данные разбиваются на 11 непересекающихся фолдов
- На каждой итерации модель обучается на 10 фолдах и валидируется на 1 оставшемся
- Это повторяется 11 раз так, чтобы каждый фолд использовался ровно один раз в качестве валидационного
- Предсказания на тестовой выборке усредняются по всем 11 фолдам
- Создаются OOF (Out-of-Fold) предсказания — предсказания модели на трейн-данных, полученные на валидационном фолде каждой итерации

### Метрика качества

**RMSLE** (Root Mean Squared Logarithmic Error) — корень из среднеквадратической логарифмической ошибки:
def RMSLE(y_true, y_pred):
return np.sqrt(mean_squared_log_error(y_true, y_pred))

**Почему RMSLE?**
- Штрафует за относительные ошибки, а не абсолютные
- Менее чувствительна к большим выбросам в данных
- Подходит для задач с неравномерным распределением таргета (например, когда большинство значений маленькие, но есть немного очень больших)

## Результаты

### Производительность кросс-валидации

После выполнения кросс-валидации модель показала следующие результаты:

| Метрика | Значение |
|---------|----------|
| **RMSLE (OOF)** | 0.0404 |
| **RMSLE (средний по фолдам)** | 0.0393 |
| **Стандартное отклонение RMSLE** | 0.0093 |

**Интерпретация:**
- OOF метрика показывает качество на полной тренировочной выборке
- Средний RMSLE по фолдам немного ниже OOF, что указывает на хорошую обобщаемость модели
- Низкое стандартное отклонение свидетельствует о стабильности модели

### Предсказания

Модель генерирует предсказания для тестовой выборки, которые затем обратно преобразуются из логарифмической шкалы: submission["charges"] = np.expm1(pred_test)
Где `expm1` выполняет обратное преобразование (exp(x) - 1).

## Структура кода
### Основные функции
#### `RMSLE(y_true, y_pred)`
Вычисляет метрику RMSLE между истинными и предсказанными значениями.
#### `cv_and_predict(df_train, df_test, train_y, model, n_splits, random_state, metric)`
Выполняет кросс-валидацию и генерацию предсказаний:

**Параметры:**
- `df_train` — тренировочные данные (признаки)
- `df_test` — тестовые данные (признаки)
- `train_y` — целевые значения для тренировочных данных
- `model` — модель машинного обучения
- `n_splits` — количество фолдов для кросс-валидации
- `random_state` — seed для воспроизводимости результатов
- `metric` — функция для вычисления метрики

**Возвращает:**
- `pred_test` — предсказания модели на тестовой выборке (усреднены по всем фолдам)
- `oof_df` — датафрейм с истинными значениями и OOF-предсказаниями на тренировочной выборке

**Внутренний процесс:**
- Печатает метрику OOF
- Печатает среднюю метрику по фолдам и её стандартное отклонение
- На каждом фолде копирует модель, обучает её, делает предсказания и усредняет по всем фолдам

### Основной пайплайн
1. **Импорт библиотек** — загрузка необходимых модулей (pandas, numpy, sklearn и т.д.)
2. **Определение RMSLE** — функция для вычисления метрики качества
3. **Загрузка данных** — чтение CSV-файлов train.csv и test.csv
4. **Подготовка таргета** — логарифмирование `charges` (log1p)
5. **Кодирование категориальных признаков** — one-hot encoding для sex, smoker, region
6. **Нормализация числовых признаков** — z-score для age и bmi
7. **Инициализация модели** — создание GradientBoostingRegressor с гиперпараметрами
8. **Кросс-валидация** — выполнение cv_and_predict
9. **Обратное преобразование** — expm1 для получения финальных значений в исходной шкале
10. **Сохранение результатов** — запись в CSV-файл submission.csv

## Файлы проекта

- **Insurance_costs.ipynb** — основной Jupyter Notebook с полным кодом
- **train.csv** 
- **test.csv**

## Возможные улучшения
**Оптимизация гиперпараметров:**
- Использование GridSearchCV или RandomizedSearchCV для поиска оптимальных параметров n_estimators и learning_rate
- Тестирование других гиперпараметров (max_depth, min_samples_split, subsample)

**Попытка других моделей:**
- XGBoost — обычно показывает лучшие результаты, чем GradientBoosting
- LightGBM — более быстрый вариант градиентного бустинга
- CatBoost — хорошо работает с категориальными переменными
- Neural Networks — глубокие нейронные сети

**Feature engineering:**
- Создание взаимодействий между признаками:
- `age * bmi` — влияние возраста и массы тела
- `age * smoker` — курение в зависимости от возраста
- `bmi * smoker` — курение в зависимости от BMI
- Полиномиальные признаки (age², bmi²)
- Группировка возраста на интервалы (age_group)

**Обработка данных:**
- Анализ выбросов и аномалий (IQR, isolation forest)
- Проверка на пропуски и корректная обработка
- Анализ распределения признаков

**Ансамблирование:**
- Stacking — обучение мета-модели на предсказаниях нескольких моделей
- Voting — голосование между несколькими моделями
- Blending — комбинирование предсказаний разных моделей с весами

**Анализ и визуализация:**
- Визуализация важности признаков (feature importance)
- Анализ остатков модели
- SHAP-значения для интерпретации предсказаний

